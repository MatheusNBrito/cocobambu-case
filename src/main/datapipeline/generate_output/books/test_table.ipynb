{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:py4j.java_gateway:GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.\n",
      "DEBUG:py4j.clientserver:Command to send: A\n",
      "4839a939a04820c0cfd5774bf42adf8259757f0dbc58baa597944016e129dc4a\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.SparkConf\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.api.java.*\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.api.python.*\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.ml.python.*\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.mllib.api.python.*\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.resource.*\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.sql.*\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.sql.api.python.*\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.sql.hive.*\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: j\n",
      "i\n",
      "rj\n",
      "scala.Tuple2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "SparkConf\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkConf\n",
      "DEBUG:py4j.clientserver:Command to send: i\n",
      "org.apache.spark.SparkConf\n",
      "bTrue\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro0\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.app.name\n",
      "sRead Data Lake Files\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro1\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.serializer.objectStreamReset\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybfalse\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.serializer.objectStreamReset\n",
      "s100\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro2\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.rdd.compress\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybfalse\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.rdd.compress\n",
      "sTrue\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro3\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybtrue\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybtrue\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybtrue\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "get\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yslocal[*]\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybtrue\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "get\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysRead Data Lake Files\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.home\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybfalse\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o0\n",
      "getAll\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yto4\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "e\n",
      "o4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yi8\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "g\n",
      "o4\n",
      "i0\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro5\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o5\n",
      "_1\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysspark.app.name\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o5\n",
      "_2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysRead Data Lake Files\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "e\n",
      "o4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yi8\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "g\n",
      "o4\n",
      "i1\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro6\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o6\n",
      "_1\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysspark.app.submitTime\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o6\n",
      "_2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ys1732627858177\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "e\n",
      "o4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yi8\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "g\n",
      "o4\n",
      "i2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro7\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o7\n",
      "_1\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysspark.rdd.compress\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o7\n",
      "_2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysTrue\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "e\n",
      "o4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yi8\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "g\n",
      "o4\n",
      "i3\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro8\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o8\n",
      "_1\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysspark.serializer.objectStreamReset\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o8\n",
      "_2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ys100\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "e\n",
      "o4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yi8\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "g\n",
      "o4\n",
      "i4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro9\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o9\n",
      "_1\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysspark.master\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o9\n",
      "_2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yslocal[*]\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "e\n",
      "o4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yi8\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "g\n",
      "o4\n",
      "i5\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro10\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o10\n",
      "_1\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysspark.submit.pyFiles\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o10\n",
      "_2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ys\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "e\n",
      "o4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yi8\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "g\n",
      "o4\n",
      "i6\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro11\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o11\n",
      "_1\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysspark.submit.deployMode\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o11\n",
      "_2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysclient\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "e\n",
      "o4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yi8\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "g\n",
      "o4\n",
      "i7\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro12\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o12\n",
      "_1\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysspark.ui.showConsoleProgress\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o12\n",
      "_2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ystrue\n",
      "DEBUG:py4j.clientserver:Command to send: a\n",
      "e\n",
      "o4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yi8\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "JavaSparkContext\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.java.JavaSparkContext\n",
      "DEBUG:py4j.clientserver:Command to send: i\n",
      "org.apache.spark.api.java.JavaSparkContext\n",
      "ro0\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Command to send: A\n",
      "4839a939a04820c0cfd5774bf42adf8259757f0dbc58baa597944016e129dc4a\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o1\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o2\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o3\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o5\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o6\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o7\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o8\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o9\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o4\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Answer received: !yro13\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o13\n",
      "sc\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro14\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o14\n",
      "conf\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro15\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "PythonAccumulatorV2\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2\n",
      "DEBUG:py4j.clientserver:Command to send: i\n",
      "org.apache.spark.api.python.PythonAccumulatorV2\n",
      "s127.0.0.1\n",
      "i56969\n",
      "s4839a939a04820c0cfd5774bf42adf8259757f0dbc58baa597944016e129dc4a\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro16\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o13\n",
      "sc\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro17\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o17\n",
      "register\n",
      "ro16\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "isEncryptionEnabled\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "isEncryptionEnabled\n",
      "ro13\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybfalse\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "getPythonAuthSocketTimeout\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "getPythonAuthSocketTimeout\n",
      "ro13\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yL15\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "getSparkBufferSize\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "getSparkBufferSize\n",
      "ro13\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yi65536\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.SparkFiles\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.SparkFiles\n",
      "getRootDirectory\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.SparkFiles\n",
      "getRootDirectory\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysC:\\\\Users\\\\Theuzao\\\\AppData\\\\Local\\\\Temp\\\\spark-9caf1c76-d5ba-4a87-8b62-dea52122d21a\\\\userFiles-01f7f2ca-a342-45cc-8240-5b91763e2202\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o15\n",
      "get\n",
      "sspark.submit.pyFiles\n",
      "s\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ys\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.util\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.util.Utils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.util.Utils\n",
      "getLocalDir\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o13\n",
      "sc\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro18\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o18\n",
      "conf\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro19\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.util.Utils\n",
      "getLocalDir\n",
      "ro19\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysC:\\\\Users\\\\Theuzao\\\\AppData\\\\Local\\\\Temp\\\\spark-9caf1c76-d5ba-4a87-8b62-dea52122d21a\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.util\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.util.Utils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.util.Utils\n",
      "createTempDir\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.util.Utils\n",
      "createTempDir\n",
      "sC:\\\\Users\\\\Theuzao\\\\AppData\\\\Local\\\\Temp\\\\spark-9caf1c76-d5ba-4a87-8b62-dea52122d21a\n",
      "spyspark\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro20\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o20\n",
      "getAbsolutePath\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysC:\\\\Users\\\\Theuzao\\\\AppData\\\\Local\\\\Temp\\\\spark-9caf1c76-d5ba-4a87-8b62-dea52122d21a\\\\pyspark-de823cb9-f640-4a5a-ad62-cee79be0ebb2\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o15\n",
      "get\n",
      "sspark.python.profile\n",
      "sfalse\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysfalse\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o15\n",
      "get\n",
      "sspark.python.profile.memory\n",
      "sfalse\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysfalse\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "getDefaultSession\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "getDefaultSession\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro21\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o21\n",
      "isDefined\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybfalse\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o13\n",
      "sc\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro22\n",
      "DEBUG:py4j.clientserver:Command to send: i\n",
      "java.util.HashMap\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yao23\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o23\n",
      "put\n",
      "sspark.app.name\n",
      "sRead Data Lake Files\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yn\n",
      "DEBUG:py4j.clientserver:Command to send: i\n",
      "org.apache.spark.sql.SparkSession\n",
      "ro22\n",
      "ro23\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro24\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "setDefaultSession\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "setDefaultSession\n",
      "ro24\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "setActiveSession\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "setActiveSession\n",
      "ro24\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o24\n",
      "version\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ys3.4.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o23\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Data Lake Files\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verificar a versão do Spark\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o24\n",
      "read\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro32\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ylo33\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o33\n",
      "add\n",
      "sC:/Projetos/cocobambu-case/src/data/lake/fact_sales.parquet\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybtrue\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "ro33\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro34\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o32\n",
      "parquet\n",
      "ro34\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !xro35\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sorg.apache.spark.sql.catalyst.parser.ParseException\n",
      "ro35\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybfalse\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "py4j\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "py4j.reflection\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "py4j.reflection.TypeUtil\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:py4j.reflection.TypeUtil\n",
      "isInstanceOf\n",
      "sorg.apache.spark.sql.AnalysisException\n",
      "ro35\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybtrue\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o35\n",
      "getMessage\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ys[PATH_NOT_FOUND] Path does not exist: file:/C:/Projetos/cocobambu-case/src/data/lake/fact_sales.parquet.\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.util\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.util.Utils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.util.Utils\n",
      "exceptionString\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.util.Utils\n",
      "exceptionString\n",
      "ro35\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/C:/Projetos/cocobambu-case/src/data/lake/fact_sales.parquet.\\r\\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1419)\\r\\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\\r\\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\\r\\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\\r\\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\\r\\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\\r\\n\tat scala.util.Success.map(Try.scala:213)\\r\\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\\r\\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\\r\\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\\r\\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\\r\\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\\r\\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\\r\\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\\r\\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\\r\\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\\r\\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\\r\\n\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o35\n",
      "getCause\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yn\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.sql\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.sql.internal\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.sql.internal.SQLConf\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.internal.SQLConf\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.internal.SQLConf\n",
      "get\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.sql.internal.SQLConf\n",
      "get\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro36\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o36\n",
      "pysparkJVMStacktraceEnabled\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybfalse\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.sql\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.sql.internal\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.sql.internal.SQLConf\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.internal.SQLConf\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.internal.SQLConf\n",
      "get\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.sql.internal.SQLConf\n",
      "get\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro37\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o37\n",
      "pysparkJVMStacktraceEnabled\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybfalse\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/Projetos/cocobambu-case/src/data/lake/fact_sales.parquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m lake_base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Projetos/cocobambu-case/src/data/lake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Ler um arquivo Parquet (Exemplo: DimStore)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dim_store_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlake_base_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/fact_sales.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Exibir as primeiras linhas e o esquema\u001b[39;00m\n\u001b[0;32m      8\u001b[0m dim_store_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\Theuzao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[0;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[0;32m    542\u001b[0m )\n\u001b[1;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Theuzao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Theuzao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/C:/Projetos/cocobambu-case/src/data/lake/fact_sales.parquet."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.sql\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.sql.internal\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yp\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "u\n",
      "org.apache.spark.sql.internal.SQLConf\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.internal.SQLConf\n",
      "DEBUG:py4j.clientserver:Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.internal.SQLConf\n",
      "get\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ym\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "z:org.apache.spark.sql.internal.SQLConf\n",
      "get\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yro38\n",
      "DEBUG:py4j.clientserver:Command to send: c\n",
      "o38\n",
      "pysparkJVMStacktraceEnabled\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !ybfalse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o33\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o29\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o30\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n",
      "DEBUG:py4j.clientserver:Command to send: m\n",
      "d\n",
      "o31\n",
      "e\n",
      "\n",
      "DEBUG:py4j.clientserver:Answer received: !yv\n"
     ]
    }
   ],
   "source": [
    "# Caminho para os arquivos no Data Lake\n",
    "lake_base_path = \"C:/Projetos/cocobambu-case/src/data/lake\"\n",
    "\n",
    "# Ler um arquivo Parquet (Exemplo: DimStore)\n",
    "dim_store_df = spark.read.parquet(f\"{lake_base_path}/fact_sales.parquet\")\n",
    "\n",
    "# Exibir as primeiras linhas e o esquema\n",
    "dim_store_df.show()\n",
    "dim_store_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgreSQL Test\") \\\n",
    "    .config(\"spark.jars\", \"C:/Projetos/postgresql-42.7.4.jar\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=log4j.properties\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Configurações do banco de dados PostgreSQL\n",
    "db_url = \"jdbc:postgresql://localhost:5432/cocobambu_case\"  # Altere o banco conforme necessário\n",
    "db_properties = {\n",
    "    \"user\": \"postgres\",  # Substitua com o seu usuário\n",
    "    \"password\": \"1475963\",  # Substitua com a sua senha\n",
    "    \"driver\": \"org.postgresql.Driver\"  # Driver JDBC do PostgreSQL\n",
    "}\n",
    "\n",
    "# Caminho para o driver JDBC PostgreSQL\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgreSQL Test\") \\\n",
    "    .config(\"spark.jars\", \"C:/Projetos/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Testando a Conexão - Lendo uma tabela existente no banco\n",
    "try:\n",
    "    # Tenta ler dados da tabela \"FactSales\"\n",
    "    test_df = spark.read.jdbc(\n",
    "        url=db_url,\n",
    "        table=\"FactSales\",  # Substitua com o nome de uma tabela que exista no banco\n",
    "        properties=db_properties\n",
    "    )\n",
    "    \n",
    "    # Exibe as primeiras 5 linhas para verificar se a leitura foi bem-sucedida\n",
    "    print(\"Conexão bem-sucedida! Exibindo as primeiras 5 linhas da tabela 'FactSales':\")\n",
    "    test_df.show(5)\n",
    "except Exception as e:\n",
    "    print(\"Erro ao conectar e ler os dados:\", e)\n",
    "\n",
    "# Testando a Escrita no Banco de Dados - Criação de uma tabela de teste\n",
    "try:\n",
    "    # Criando um DataFrame de teste simples\n",
    "    test_data = [(\"João\", 25), (\"Maria\", 30), (\"José\", 28)]\n",
    "    columns = [\"nome\", \"idade\"]\n",
    "    test_df = spark.createDataFrame(test_data, columns)\n",
    "\n",
    "    # Escrevendo os dados na tabela \"TestTable\" no banco de dados PostgreSQL\n",
    "    test_df.write.jdbc(\n",
    "        url=db_url,\n",
    "        table=\"TestTable\",  # Nome da tabela de teste\n",
    "        mode=\"overwrite\",  # Use \"overwrite\" para sobrescrever ou \"append\" para adicionar\n",
    "        properties=db_properties\n",
    "    )\n",
    "\n",
    "    print(\"Dados de teste escritos na tabela 'TestTable' com sucesso!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Erro ao gravar os dados no banco de dados:\", e)\n",
    "\n",
    "# Finaliza a sessão Spark\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
